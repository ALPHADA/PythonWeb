### 딥러닝 개요

안녕하세요, 오늘은 머신러닝에 대해 알아보는 시간을 가져보겠습니다. 2016년에 구글 딥마인드가 이세돌과의 바둑 대결에서 승리한 이후로, 딥러닝과 인공지능의 발전은 더욱 가속화되었습니다. 이제는 일상에서 딥러닝 기술을 쉽게 접할 수 있는 시대가 되었습니다. 예를 들어, 구글 번역은 딥러닝을 사용하여 언어를 번역하고, TTS(Text-to-Speech) 엔진은 텍스트를 입력하면 컴퓨터가 사람처럼 소리로 변환해주는 기술입니다. 실제로, 딥러닝을 적용한 TTS는 사람처럼 자연스러운 발음을 구현하고 있습니다.

마이크로소프트는 딥러닝을 활용하여 코드를 자동으로 작성하는 AI를 개발했습니다. 예를 들어, 스케치 투 코드(Sketch2Code)라는 실험적인 사이트에서는 사용자가 스케치를 업로드하면 이를 기반으로 HTML 코드를 자동으로 생성해줍니다. 또한, 파이썬 코드로 문제를 해결하는 AI도 개발되었습니다. 어도비는 '프로젝트 보이스'라는 이름으로 딥러닝을 이용해 포토샵에서 얼굴 보정 여부를 판단하고 원래 사진으로 되돌리는 AI를 발표했습니다.

딥러닝은 무엇일까요? 쉽게 설명하자면, 딥러닝은 머신러닝의 한 분야로, 컴퓨터에게 학습을 시키는 과정입니다. 예를 들어, 개와 고양이 사진을 구분하는 문제를 다룰 때, 딥러닝은 수백만 장의 사진을 학습하여 개와 고양이의 특징을 파악하게 됩니다. 이를 통해 컴퓨터가 새로운 사진을 보고 개인지 고양이인지 구분할 수 있게 되는 것이죠.

기존의 머신러닝은 사전 지식이 필요하며, 데이터를 가이드라인에 맞춰 정리해주어야 했습니다. 반면에 딥러닝은 이러한 가이드라인 없이도 컴퓨터가 데이터를 통해 학습하고 패턴을 찾아냅니다. 예를 들어, 알파고는 수많은 바둑 대국을 학습하여 바둑 두는 법을 스스로 터득했습니다. 딥러닝의 가장 큰 장점은 사전 지식 없이도 다양한 분야에 적용할 수 있다는 것입니다. 이는 바둑, 유방암 진단, 자율주행 등 다양한 분야에서 활용되고 있습니다.

딥러닝의 기본 개념은 신경망(Neural Network)을 기반으로 합니다. 신경망은 인간의 뇌와 유사한 구조로, 뉴런과 시냅스를 모방하여 설계되었습니다. 이를 통해 컴퓨터가 데이터를 학습하고, 새로운 문제를 해결할 수 있습니다. 딥러닝을 통해 이미지 인식, 음성 인식, 번역 등 다양한 문제를 해결할 수 있으며, 이는 자율주행, 번역 등 실생활에서 유용하게 사용되고 있습니다.

앞으로의 강의에서는 딥러닝의 기초 이론과 실제 예제들을 다룰 예정입니다. 강의는 기본적인 수학 개념을 다룰 것입니다. 파이썬을 사용한 코드 예제도 함께 제공할 예정이니, 파이썬을 미리 공부해 오시면 좋겠습니다.

### 머신러닝 개념

머신러닝은 기본적으로 기계가 스스로 학습하고 예측하는 기술입니다. 예를 들어, 사진을 보고 사람이 있는지 아닌지를 판단하는 문제를 생각해봅시다. 사람은 눈, 코, 입 등을 보고 사람이네 하고 판단합니다. 기계도 비슷하게 특정 알고리즘을 사용해 판단하게 됩니다. 이런 과정을 머신러닝이라고 합니다.

예를 들어, 스팸 메일을 자동으로 분류하는 AI를 만들 때, 과거에는 사람이 직접 알고리즘을 만들었습니다. 예를 들어, "제목에 '광고'라는 단어가 있으면 스팸으로 분류해라" 같은 단순한 규칙을 적용했죠. 하지만 머신러닝을 사용하면, 기계가 스스로 수천, 수만 개의 이메일을 분석하면서 스팸 메일과 관련된 패턴을 찾아내고, 그에 맞는 알고리즘을 자동으로 생성할 수 있습니다.

머신러닝은 크게 세 가지 종류로 나뉩니다:

- 지도 학습 (Supervised Learning): 정답이 있는 데이터를 이용해 학습합니다. 예를 들어, 개와 고양이 사진이 있고, 각각의 사진에 "개" 또는 "고양이"라는 라벨이 붙어 있다면, 기계는 이를 학습하고 새로운 사진에 대해서도 개와 고양이를 구분할 수 있게 됩니다.

- 비지도 학습 (Unsupervised Learning): 정답이 없는 데이터를 이용해 학습합니다. 수많은 데이터를 가지고 기계가 스스로 유사한 것끼리 분류하거나 군집화하는 과정을 통해 패턴을 발견합니다. 예를 들어, 영화 추천 시스템이 있습니다. 기계가 영화를 비슷한 장르나 스타일로 묶어 추천하는 것이 이에 해당합니다.

- 강화 학습 (Reinforcement Learning): 보상과 벌을 통해 학습하는 방법입니다. 예를 들어, 벽돌깨기 게임을 가르친다고 하면, 처음에는 기계가 무작위로 조작하다가 공을 놓치는 경우도 있지만, 점점 점수를 높이는 방향으로 학습하게 됩니다.

이렇게 기계가 스스로 학습하고 예측하는 것을 머신러닝이라고 하며, 이를 통해 다양한 문제를 해결할 수 있습니다.

이제 예시를 하나 들어보겠습니다. 예를 들어, 6월 모의고사와 9월 모의고사 성적을 기반으로 수능 점수를 예측한다고 해봅시다. 사람이 직접 "6월 성적에 0.5를 곱하고, 9월 성적에 0.5를 곱해 더하면 수능 점수가 될 것이다"라고 예측할 수 있습니다. 이때 0.5라는 값은 가중치(weight)라고 부르며, 머신러닝에서는 기계가 이러한 가중치를 자동으로 조정하도록 학습시킬 수 있습니다.

이 가중치를 조정하는 방법은, 실제 데이터를 바탕으로 기계가 예측한 값과 실제 값의 차이를 최소화하는 방향으로 조정해 나가는 것입니다. 이를 통해 보다 정확한 예측 모델을 만들 수 있습니다.

마지막으로, 딥러닝에 대해 간단히 소개하겠습니다. 딥러닝은 여러 층(layer)의 뉴럴 네트워크를 사용해 복잡한 패턴을 학습하는 방법입니다. 뉴럴 네트워크는 입력 데이터를 여러 계층을 통해 처리하여 최종 출력을 생성합니다. 딥러닝을 통해 더욱 정교하고 복잡한 문제를 해결할 수 있습니다.

다음 시간에는 딥러닝의 활용과 장점에 대해 자세히 알아보겠습니다.

### 뇌를 본딴 뉴럴네트워크 (컴퓨터가 사람처럼 생각하겠냐?)

지난 시간에 우리는 머신러닝 모델을 만들었고, 이를 통해 가중치(w) 값을 찾는 과정을 설명했습니다. 오늘은 더 복잡하고 정교한 예측을 위해 사용되는 모델에 대해 알아보겠습니다.

우리는 예측 모델을 만들 때, 여러 개의 직선으로 연결된 단순한 모델을 폴리셉트론(perceptron)이라고 부릅니다. 이는 오래된 이론으로, 복잡한 예측을 위해 발전된 형태로 사용할 수 있습니다. 그 아이디어는 기계가 사람처럼 생각하게 할 수 있지 않을까 하는 가정에서 시작되었습니다.

인간의 뇌는 뉴런과 시냅스를 통해 신호를 전달하며 동작합니다. 머신러닝 모델도 이를 본따 뉴럴 네트워크를 사용합니다. 물론 뇌를 완전히 모방한 것은 아니지만, 이 방식은 실제로 좋은 결과를 가져다줍니다. 그래서 뉴럴 네트워크가 유행하게 되었습니다.

뉴럴 네트워크는 입력 데이터를 여러 계층(layer)으로 나누어 처리하며, 이 과정을 통해 더 복잡하고 정교한 예측을 할 수 있습니다. 이를 딥러닝이라고도 부릅니다. 딥러닝에서는 중간 계층인 '생각 레이어'가 생각을 저장하고 추론을 돕습니다. 예를 들어, 수능 성적을 예측할 때, 4월부터 9월까지의 모의고사 성적을 이용한다고 합시다.

우리는 4월부터 6월 성적이 수능 성적에 미치는 영향이 적다고 생각할 수 있습니다. 반면 7월부터 9월 성적은 수능과 더 관련이 있을 것입니다. 따라서 각 기간의 평균 성적을 구하고, 이를 토대로 수능 성적을 예측할 수 있습니다. 이를 머신러닝 모델로 구축하면 기계가 수많은 시도를 통해 스스로 예측을 개선하게 됩니다.

예를 들어, 컴퓨터가 사람 얼굴 사진을 보고 사람인지 아닌지 예측하는 문제를 생각해봅시다. 사람이라면 눈, 코, 입 등을 보고 판단하지만, 컴퓨터는 뉴럴 네트워크를 통해 사진의 특징을 추출하고 판단합니다. 이를 통해 더욱 정확하게 예측할 수 있습니다.

뉴럴 네트워크는 사진의 각 부분을 분석하여 눈, 코, 입 같은 특징을 자동으로 추출합니다. 이런 과정을 '피처 추출(feature extraction)'이라고 합니다. 전통적인 머신러닝에서는 사람이 직접 이러한 특징을 정의하고 가이드를 제공해야 했지만, 딥러닝에서는 이러한 과정이 필요하지 않습니다. 대신 더 많은 데이터가 필요합니다.

이러한 이유로 뉴럴 네트워크를 사용하면 더 나은 예측 결과를 얻을 수 있습니다. 특히 시퀀스 데이터나 이미지 데이터와 같은 문제에서 딥러닝의 성능이 뛰어납니다.

다음 시간에는 뉴럴 네트워크의 구체적인 수치와 예측 방법에 대해 더 자세히 알아보겠습니다.

### 손실함수 (loss function) 로 컴퓨터에게 오차를 알려줘야함

지난 시간에 히든 레이어를 도입하면 더 정교한 예측이 가능하다고 했습니다. 이번에는 6월 성적과 2월 성적을 입력해 수능 점수를 예측하는 모델을 히든 레이어를 사용하여 다시 계산해 보겠습니다.

우선, 히든 레이어의 구성 요소를 설명하겠습니다. 각 요소는 '노드'라고 부르며, 이 노드들은 숫자로 표현됩니다. 노드의 값은 이전 노드들과의 연결을 통해 계산됩니다. 이때 각 연결에 가중치 
w 값을 곱해 더하는 방식으로 계산합니다.

예를 들어, 6월 성적과 2월 성적이 모델에 입력되면, 각각의 입력에 대응하는 가중치 w1, w2 를 곱해 더한 값을 노드에 입력합니다. 이는 새로운 노드가 추가되더라도 동일한 방식으로 적용됩니다. 모든 노드를 통해 계산된 값들이 히든 레이어를 통해 전달되고, 이를 통해 최종 예측 결과가 도출됩니다.

여기서 가중치 w 값은 컴퓨터가 학습을 통해 스스로 찾아내는 값입니다. 우리는 미지수로 표현하지만, 실제로는 컴퓨터가 학습 과정을 통해 최적의 값을 찾게 됩니다. 히든 레이어는 '중간 단계'로, 예측 과정에서 중간 결과를 저장하고 계산하는 역할을 합니다.

이제, 예측된 값과 실제 값의 차이를 오차(error)라고 합니다. 오차를 계산하는 방법은 간단합니다. 예를 들어, 학생 1, 2, 3의 6월 성적과 9월 성적, 그리고 실제 수능 점수를 가지고 오차를 계산할 수 있습니다. 예측된 수능 점수와 실제 점수의 차이를 구하고, 이를 통해 각 학생의 오차를 합산하여 총 오차를 계산합니다.

그러나 오차 계산 시, 양수와 음수의 차이가 상쇄될 수 있으므로, 절대값을 사용하거나 제곱을 사용해 오차를 계산합니다. 이를 통해 더욱 정확한 오차를 구할 수 있습니다. 이러한 방식을 '평균 제곱 오차(MSE)'라고 부릅니다. 오차를 최소화하는 방향으로 가중치를 조정하도록 컴퓨터에게 명령을 내리면 됩니다.

이때, '로스 함수' 또는 '코스트 함수'를 사용하여 모델의 정확도를 평가합니다. 로스 함수는 예측값과 실제값의 차이를 측정하는 함수로, 모델이 얼마나 정확한지 평가하는 데 사용됩니다. 이는 텐서플로와 같은 라이브러리를 사용하여 쉽게 구현할 수 있습니다.

이번 강의에서는 히든 레이어를 통한 수능 점수 예측 방법과 로스 함수의 역할에 대해 설명했습니다. 하지만 히든 레이어가 있든 없든 결과가 같을 수 있다는 중대한 문제가 존재합니다. 다음 시간에는 이 문제에 대해 더 깊이 탐구해 보겠습니다.


### 활성함수가 없으면 뉴럴네트워크가 아님 (Activation Function)
여기까지 설명한 내용에는 중대한 결함이 하나 있습니다. 히든 레이어가 있든 없든 결과가 거의 동일하게 나온다는 점입니다. 실제 실험을 해보거나 머신러닝 모델을 돌려보면, 히든 레이어의 유무에 상관없이 결과가 크게 달라지지 않을 수 있습니다.

이유는 간단합니다. 히든 레이어가 없는 경우, 인풋과 아웃풋이 단순히 수식을 통해 연결됩니다. 예를 들어, 6월 성적과 9월 성적에 가중치를 곱해 더하면 최종 예측 값이 나옵니다. 반면, 히든 레이어가 있는 경우에도, 입력값에 가중치를 곱하고 더하는 방식은 동일합니다. 다만, 중간에 히든 레이어를 거치면서 새로운 가중치를 적용할 뿐입니다. 결과적으로, 히든 레이어가 있든 없든 수식적으로는 동일한 형태의 예측 결과가 나오게 됩니다.

이를 해결하기 위해 도입된 개념이 바로 **활성화 함수(Activation Function)**입니다. 히든 레이어에서 계산된 값에 활성화 함수를 적용하면, 단순히 숫자를 계산하는 것이 아니라, 값을 비선형적으로 변환하여 더욱 복잡한 모델링이 가능해집니다.

가장 일반적인 활성화 함수는 시그모이드 함수입니다. 이 함수는 입력값을 0과 1 사이의 값으로 변환합니다. 예를 들어, 입력값이 0이면 출력은 0.5, 양수일 경우 1에 가까워지고, 음수일 경우 0에 가까워집니다.

다른 활성화 함수로는 **하이퍼볼릭 탄젠트 함수(Tanh)**가 있습니다. 이 함수는 입력값을 -1과 1 사이로 변환합니다. 또 다른 예로는 **렐루 함수(ReLU)**가 있는데, 양수는 그대로 유지하고, 음수는 0으로 변환하는 함수입니다.

활성화 함수의 도입으로 인해 히든 레이어는 더 이상 단순한 계산 과정이 아니라 비선형적인 변환을 수행하게 됩니다. 이를 통해 모델은 더 복잡한 패턴을 학습하고 예측할 수 있습니다. 따라서 활성화 함수는 비선형성을 도입해 더 정교한 예측을 가능하게 하는 중요한 요소입니다.

마지막으로, 모든 레이어에 활성화 함수가 필요한 것은 아니지만, 출력 레이어에는 상황에 따라 활성화 함수를 적용하지 않을 수 있습니다. 예를 들어, 출력값을 0과 1 사이로 압축해야 하는 경우, 시그모이드 함수를 사용할 수 있습니다.

이것이 바로 뉴럴 네트워크 구성의 핵심입니다. 다음 시간에는 학습 과정에서 가중치 업데이트를 어떻게 하는지, 즉 역전파(Backpropagation) 알고리즘에 대해 다루겠습니다. 이는 머신러닝 모델의 성능을 높이는 중요한 개념입니다. 이 원리를 이해하면 컴퓨터가 어떻게 최적의 가중치를 찾아가는지 알 수 있을 것입니다.

### 신나는 경사하강법
지난 시간까지 강의를 잘 들으셨다면, 이제 뉴럴 네트워크를 통해 예측 값을 계산하는 방법을 이해하셨을 겁니다. 예를 들어, 6월 성적과 같은 데이터를 입력하면, 가중치 w 를 곱하여 h1 값을 만들어냅니다. 그러나 이 값을 그대로 다음으로 전달하는 것이 아니라, 시그모이드 함수와 같은 활성화 함수에 넣어 변환합니다. 이를 통해 변환된 값을 다음 노드로 전달하며, 최종 예측 값을 도출합니다.

이 예측 값을 실제 값과 비교하여 오차를 계산하고, 오차를 최소화하는 가중치 w 값을 찾으면 모델의 정확도가 높아집니다. 컴퓨터가 이 과정을 통해 최적의 w 값을 찾는 방법을 간단히 설명해드리겠습니다.

예를 들어, w1 하나만 생각해봅시다. w1, w2 외에도 여러 가중치가 있지만, 단순화를 위해 w1만 고려하겠습니다. 만약 w1 에 다양한 값을 대입해보면, 총 손실 값이 어떻게 변하는지 알 수 있습니다. 예를 들어, w1 의 값이 3일 때의 총 손실을 계산한 후, 손실 값을 최소화하는 방향으로 w1 을 조정해야 합니다. 이를 위해 경사 하강법(Gradient Descent) 이라는 알고리즘을 사용합니다.

경사 하강법은 가중치가 증가하거나 감소하는 방향을 결정하는 방법입니다. 이를 위해 현재 w 값에서 접선의 기울기를 계산합니다. 예를 들어, w1 이 3일 때 기울기가 1이라면, w1 을 감소시키는 방향으로 조정해야 합니다. 이 과정은 단순히 기울기를 빼는 것이 아니라, 러닝 레이트(Learning Rate) 라는 상수를 곱하여 조정합니다. 러닝 레이트는 가중치 조정의 크기를 결정하며, 너무 크면 최적점을 지나칠 수 있고, 너무 작으면 학습이 느려질 수 있습니다.

이 과정을 반복하면서 최적의 w 값을 찾아내는 것이 머신 러닝의 학습 과정입니다. 최적의 w 값을 찾기 위해 경사 하강법을 사용하는데, 이때 사용되는 러닝 레이트는 적절히 조정해야 합니다. 이는 실험적으로 설정하며, 일반적으로 0.01 또는 0.001과 같은 소숫점 값을 사용합니다.

또한, 러닝 레이트는 일정하게 유지하는 대신, 학습이 진행됨에 따라 동적으로 조정할 수 있습니다. 이를 위해 옵티마이저(Optimizer) 라는 알고리즘을 사용합니다. 예를 들어, 모멘텀(Momentum) 은 관성을 적용하여 빠르게 수렴할 수 있도록 도와주며, 아다그라드(Adagrad) 와 아담(Adam) 은 학습 속도를 자동으로 조정해주는 알고리즘입니다. 이 중에서도 아담은 대부분의 상황에서 좋은 결과를 제공하는 편입니다.

마지막으로, 기울기라는 개념을 2차원 그래프에서 설명했지만, 실제로는 고차원 공간에서 이루어집니다. 이로 인해 직접적으로 시각화하기 어려운 경우가 많습니다. 따라서, 보다 정확하게 표현하자면, w1 의 작은 변화가 손실 값에 얼마나 영향을 미치는지를 계산하여 빼는 것입니다. 이러한 과정은 텐서플로와 같은 라이브러리가 자동으로 처리해주므로, 코드를 작성하는 데 직접적으로 계산할 필요는 없습니다.

이 강의에서는 뉴럴 네트워크의 학습 과정과 경사 하강법의 기본 개념을 설명했습니다. 

### Tensorflow 2 기초 빠르게 정리

오늘은 텐서플로우의 기초 중의 기초인 텐서(Tensor) 라는 자료형에 대해 알아보겠습니다. 텐서플로우가 설치되어 있다면, 파이썬 파일을 하나 만들어 텐서에 대해 실습해보겠습니다.

먼저, 텐서라는 변수를 만들어보겠습니다. tf.constant라는 함수를 사용하여 다음과 같이 작성해보세요:

```python
import tensorflow as tf

tensor = tf.constant([1, 2, 3])
print(tensor)
```
이 코드를 저장하고 터미널에서 python tensor.py라고 입력하여 파일을 실행해보면, 텐서의 출력 결과가 나옵니다. 텐서플로우 실행 시 처음에는 경고 메시지가 나올 수 있지만, 에러만 없다면 문제 없습니다.

이렇게 만든 텐서는 텐서플로우에서 가장 기본적인 자료형입니다. 파이썬의 리스트나 숫자 자료형과 유사하지만, 텐서플로우에서는 이를 텐서라고 부릅니다. 텐서를 만들 때는 숫자나 리스트를 포함하여 여러 가지 형식을 사용할 수 있습니다.

텐서를 배워야 하는 이유는 간단합니다. 머신러닝에서는 다양한 연산을 수행해야 하며, 특히 많은 데이터와 가중치를 다룰 때 매우 유용합니다. 예를 들어, 입력값 x1,x2, x3에 각각의 가중치 w1, w2, w3를 곱하여 새로운 노드를 만드는 과정에서 수많은 연산이 필요합니다. 이때, 텐서를 이용하면 복잡한 연산을 간단하게 수행할 수 있습니다.

텐서는 행렬과 유사한 구조를 가지며, 여러 숫자를 한 번에 처리할 수 있습니다. 예를 들어, 다음과 같이 텐서 두 개를 더해보겠습니다:

```python
tensor1 = tf.constant([1, 2, 3])
tensor2 = tf.constant([4, 5, 6])
result = tf.add(tensor1, tensor2)
print(result)
```
이 코드를 실행하면 각각의 요소가 더해진 결과를 출력합니다. 텐서 연산은 이렇게 간단하게 수행할 수 있습니다. 텐서 연산에는 더하기, 빼기, 곱하기 등의 다양한 연산이 포함됩니다.

다음으로, 다차원 텐서에 대해 알아보겠습니다. 텐서는 리스트 안에 리스트를 포함하여 다차원 구조를 가질 수 있습니다. 예를 들어, 다음과 같이 2차원 텐서를 만들 수 있습니다:

```python
tensor3 = tf.constant([[1, 2], [3, 4]])
print(tensor3)
```
이 코드는 2행 2열의 텐서를 생성합니다. 이처럼 텐서는 다양한 차원과 모양으로 만들 수 있습니다. 텐서의 모양(shape)을 확인하기 위해 다음과 같이 코드를 작성할 수 있습니다:

```python
print(tensor3.shape)
```
이 코드를 실행하면 텐서의 차원을 알려주는 (2, 2)와 같은 결과가 출력됩니다. 이는 2개의 차원과 각 차원의 크기를 의미합니다. 텐서의 모양은 데이터 분석 시 매우 중요한 정보입니다.

또한, 텐서의 데이터 타입도 중요한 요소입니다. 텐서의 데이터 타입은 dtype 속성을 통해 확인할 수 있습니다. 예를 들어, 정수를 저장하면 int32, 실수를 저장하면 float32와 같이 나타납니다. 보통 딥러닝에서는 실수 자료형을 주로 사용합니다.

마지막으로, 변수(Variable) 라는 특별한 텐서 유형에 대해 알아보겠습니다. 변수를 사용하면 텐서의 값을 변경할 수 있습니다. 예를 들어, 가중치와 같은 값을 저장하고 싶을 때 변수를 사용할 수 있습니다.

```python
variable = tf.Variable([1.0, 2.0, 3.0])
print(variable)
```
변수는 tf.constant와는 달리 값을 수정할 수 있으며, variable.assign(new_value)를 통해 값을 변경할 수 있습니다.

지금까지 텐서플로우의 기본적인 텐서 자료형에 대해 알아보았습니다. 이 내용은 딥러닝과 뉴럴 네트워크의 기초를 이해하는 데 중요한 역할을 합니다. 


### Tensorflow 2로 해보는 간단한 Linear Regression 선형회귀 예측

이제 본격적으로 실전 프로젝트로 넘어가기 전에, 텐서플로우에서 딥러닝이 어떤 식으로 이루어지는지 간단한 수학 문제를 통해 설명해 드리겠습니다. 예를 들어, 사람들의 키와 신발 사이즈 데이터를 수집했다고 가정해봅시다. 각 사람의 키와 신발 사이즈가 다음과 같이 주어졌습니다:
```
키: 170cm, 신발 사이즈: 260
키: 180cm, 신발 사이즈: 270
...
```

이 데이터를 기반으로, 키와 신발 사이즈 사이의 관계를 찾아보겠습니다. 우리는 다음과 같은 식을 세울 수 있습니다:

`신발 사이즈 = a×키+b`

여기서 a와 b는 찾고자 하는 미지수입니다. 예를 들어, 키가 170cm인 사람이 신발 사이즈가 260이라고 한다면, 우리는 이 데이터를 바탕으로 a와 b를 구할 수 있습니다.

이 문제를 풀기 위해, 다음과 같이 간단한 모델을 설정해보겠습니다. 키가 170cm이고, 신발 사이즈가 260인 데이터를 사용하여 예측 모델을 만들어보겠습니다.

먼저, 텐서플로우에서 변수를 정의합니다:

```python
import tensorflow as tf

a = tf.Variable(0.1)
b = tf.Variable(0.2)
```
tf.Variable을 사용하여 초기값을 설정하고, 경사 하강법을 통해 학습을 진행할 수 있도록 합니다. 이제 옵티마이저를 정의해보겠습니다:

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=0.15)
```
Adam 옵티마이저는 경사 하강법을 이용하여 변수들을 업데이트해줍니다. 이어서, 손실 함수를 정의합니다. 손실 함수는 예측 값과 실제 값 사이의 오차를 계산하는 함수입니다. 여기서는 간단히 평균 제곱 오차(MSE)를 사용합니다:

```python
def loss_fn():
    y_pred = a * 170 + b
    y_true = 260
    return tf.square(y_pred - y_true)
```
이제 경사 하강법을 적용하여 학습을 진행합니다. 이를 위해 옵티마이저의 apply_gradients 메소드를 사용합니다:

```python
for _ in range(300):
    optimizer.minimize(loss_fn, var_list=[a, b])
    print(a.numpy(), b.numpy())
```
이 코드는 300번의 반복을 통해 최적의 a와 b 값을 찾습니다. 최종적으로, a와 b 값이 업데이트되면서 점점 더 정확한 예측 모델이 형성됩니다.

이렇게 학습이 완료되면, 우리는 새로운 키 값을 입력하여 신발 사이즈를 예측할 수 있습니다. 예를 들어, 키가 180cm인 사람의 신발 사이즈를 예측해보면 다음과 같습니다:

```python
predicted_shoe_size = a.numpy() * 180 + b.numpy()
print(predicted_shoe_size)
```
이렇게 간단한 선형 모델을 사용하여 키와 신발 사이즈 사이의 관계를 예측할 수 있습니다. 물론, 실제로는 더 복잡한 모델과 데이터를 다루게 되지만, 이 예제는 기본적인 개념을 이해하는 데 도움을 줄 것입니다.

이후에는 더 많은 데이터를 사용하여 보다 복잡한 모델을 학습시키는 방법을 다룰 예정입니다. 이 강의는 가벼운 마음으로 참고하시고, 더 진보된 딥러닝 모델에 대해 배울 준비를 해보세요.

### 대학 합격 예측 AI 만들기 (풀버전)
#### (Part 1) Keras로 모델 만들기
안녕하세요! 첫 번째 프로젝트로 딥러닝 모델을 만들어보겠습니다. 이번에는 학점과 영어 성적 데이터를 이용해 대학원 입학 여부를 예측해보려고 합니다. 첨부 파일을 보시면 여러 사람의 대학원 입학 정보가 있습니다. 예를 들어, 어떤 사람은 영어 성적이 380점이고 학점이 3.2점이며 랭크 3인 대학교에 지원했다고 가정합시다. 그리고 '어드미션'이라는 열에는 이 사람이 합격했는지 여부가 기록되어 있습니다. 1은 합격, 0은 불합격을 의미합니다.

이 데이터를 이용해 학습을 시키고, 새로운 학생이 주어진 조건에서 입학할 확률을 예측하는 모델을 만들어보겠습니다. 목표는 학점과 영어 성적, 대학교 랭크를 입력하면 합격 확률을 예측하는 것입니다.

1. 프로젝트 설정
먼저, Python 파일을 생성하고 필요한 라이브러리를 임포트합니다:

```python
import tensorflow as tf
from tensorflow.keras import layers
```
Sequential 쓰면 신경망 레이어를 쉽게 만들어줍니다.   
Keras는 텐서플로우에 포함된 고수준 API로, 딥러닝 모델을 쉽게 만들 수 있게 도와줍니다. 이제 딥러닝 모델을 구성해보겠습니다. 모델은 여러 개의 레이어로 구성되며, 각 레이어는 노드로 구성됩니다. 이때, 각 레이어의 노드 수는 사용자가 지정할 수 있습니다.

2. 모델 구성
```python
model = tf.keras.Sequential([
    layers.Dense(64, activation='relu'), # 레이어 갯수/노드 갯수 적절히 -> 결과 잘 나올때까지 실험으로 파악
    layers.Dense(128, activation='relu'), # 2의 제곱수를 관용적으로 많이 사용함
    layers.Dense(1, activation='sigmoid')
])
```

위 코드에서 첫 번째 레이어는 64개의 노드로 구성되고, 두 번째 레이어는 128개의 노드로 구성됩니다. 마지막 레이어는 1개의 노드로 구성되며, 0과 1 사이의 확률 값을 출력하는 sigmoid 활성화 함수를 사용합니다.

3. 모델 컴파일
모델을 컴파일할 때는 옵티마이저, 손실 함수, 메트릭스를 정의합니다.

```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```
옵티마이저: 경사 하강법을 사용해 모델을 학습시킵니다. adam 옵티마이저는 일반적으로 좋은 성능을 보입니다.
- 경사하강법으로 w 값을 구해야하는데, 여기서 기울기만큼 빼서 왔다갔다하는데 기울기를 뺄때 러닝웨이트를 곱해서 뺀다고 했음. 옵티마이저는 이때 적절한 값을 수정하면서 곱하도록 만들어 줌.
손실 함수: binary_crossentropy를 사용해 0과 1 사이의 확률을 예측합니다.
메트릭스: 모델의 성능을 평가하는 기준으로, 여기서는 정확도(accuracy)를 사용합니다.

4. 모델 학습 
이제 모델을 학습시킵니다. 데이터를 준비하여 학습시키는 방법은 다음과 같습니다.

```python
model.fit( x데이터, y데이터, epochs=100)

# x데이터 = [ [학생1데이터], [학생2데이터], ... ]
# x데이터 = [ [380, 3.21, 3], [660, 3.67, 3], [], ... ]
# y데이터 = [ 정답1, 정답2, 정답3, ...]
# y데이터 = [ [0], [1], [1], [1], ...]
```
- x데이터: 학습 데이터
- y데이터: 실제 정답
- epochs: 몇 번 학습할지 정하는 변수

#### (Part 2) 데이터 준비하기
1. 데이터 로드
```python
import pandas as pd

data = pd.read_csv('admission_data.csv')
```
- pandas : 파이썬에서 데이터 조작 및 분석을 위한 라이브러리. 데이터 프레임을 사용하여 데이터 전처리와 분석을 쉽게 수행할 수 있습니다.
- 파이썬으로 엑셀처럼 데이터를 다루고 싶을때 사용합니다.

Pandas를 사용하여 CSV 파일을 읽어옵니다. CSV 파일에는 학점, 영어 성적, 대학교 랭크, 입학 여부 등의 정보가 담겨 있습니다.

2. 데이터 전처리
데이터를 학습시키기 전에 결측치를 처리하고 필요한 부분을 추출합니다.

```python
data = data.dropna()
```
이 코드는 데이터에서 결측치가 있는 행을 제거합니다. 필요에 따라 결측치를 특정 값으로 채울 수도 있습니다.

3. 데이터 준비
모델에 입력할 X 데이터와 출력할 y 데이터를 준비합니다.

```python
# x데이터 = [ [학생1데이터], [학생2데이터], ... ]
# x데이터 = [ [380, 3.21, 3], [660, 3.67, 3], [], ... ]
# y데이터 = [ 정답1, 정답2, 정답3, ...]
# y데이터 = [ [0], [1], [1], [1], ...]

y = data['admit'].values
print(y)

X = []
for i, rows in data.iterrows(): # data 라는 dataframe 을 가로 한 줄씩 출력하라
  print(rows)
  print(rows['gre'])

```

```python
X = data[['GRE Score', 'GPA', 'Rank']].values
y = data['Admission'].values
```
여기서는 'GRE Score', 'GPA', 'Rank'를 입력 데이터로 사용하고, 'Admission'을 출력 데이터로 사용합니다.

### (Part 3) 학습시키기 & 예측해보기
1. 모델 학습
이제 준비된 데이터를 사용해 모델을 학습시킵니다.

```python
model.fit(X, y, epochs=100)
```
epochs는 전체 데이터셋을 몇 번 반복해서 학습할 것인지를 결정합니다. 여기서는 100번 학습합니다.

- 넘파이(numpy) : 파이썬에서 행렬 및 대규모 다차원 배열을 다루기 위한 라이브러리. 과학 연산을 위한 다양한 기능을 제공합니다.


2. 예측
모델이 학습된 후, 새로운 데이터를 입력하여 예측을 수행할 수 있습니다.

```python
new_data = np.array([[750, 3.7, 3], [400, 2.1, 1]])
predictions = model.predict(new_data)
print(predictions)
```
predict 메서드를 사용해 새로운 데이터의 입학 확률을 예측합니다. 예를 들어, 첫 번째 데이터는 GRE 750, GPA 3.7, 랭크 3인 학생이 61% 확률로 합격할 것이라고 예측합니다. 두 번째 데이터는 낮은 확률로 불합격할 가능성이 큽니다.

이처럼, Keras와 텐서플로우를 사용해 간단한 딥러닝 모델을 만들고, 학습시키고, 예측할 수 있습니다. 데이터 전처리, 하이퍼파라미터 튜닝 등의 추가적인 작업을 통해 모델의 성능을 더욱 향상시킬 수 있습니다. 이를 통해 여러분은 보다 정확한 예측을 할 수 있는 모델을 만들 수 있습니다.
